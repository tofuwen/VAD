{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VAD-dlrm.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNID8JzIY/cCV6m1qTan07Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Instructions\n","\n","1. git clone the repo and upload the whole repo to google drive\n","2. use Google Colab to execute the code (including model training, inference, and applying VAD)"],"metadata":{"id":"f5ER6W4Hp_AE"}},{"cell_type":"markdown","source":["# Section 1: DLRM training && inference"],"metadata":{"id":"wPC0bWehrFGa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KAcJ8H2lVPin"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["# enter your working directory\n","\n","# %cd /content/gdrive/MyDrive/your_folder/VAD/dlrm/"],"metadata":{"id":"sT7qT0uOVgib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# install the required package\n","\n","# !pip install \"git+https://github.com/mlperf/logging.git@beaf26d\""],"metadata":{"id":"f2ros_RKmD4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# command to train DLRM model && do inference on test / validation dataset\n","# to fully reproduce the result, please train the model on 15M data instead of 100k example data\n","\n","\n","# %%shell\n","\n","# for VAR in {1..1}\n","# do \n","#     python dlrm_s_pytorch.py \\\n","#         --arch-sparse-feature-size=16 \\\n","#         --arch-mlp-bot=\"13-512-256-64-16\" \\\n","#         --arch-mlp-top=\"512-256-1\" \\\n","#         --data-generation=dataset \\\n","#         --data-set=kaggle \\\n","#         --raw-data-file=./input/train_last_100k_as_example.txt \\\n","#         --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz \\\n","#         --save-model=./model/model_example.pt \\\n","#         --numpy-rand-seed=${VAR} \\\n","#         --data-randomize=total \\\n","#         --lr-num-warmup-steps=0 \\\n","#         --lr-decay-start-step=0 \\\n","#         --loss-function=bce \\\n","#         --round-targets=True \\\n","#         --mlperf-logging \\\n","#         --nepochs=2 \\\n","#         --learning-rate=0.1 \\\n","#         --mini-batch-size=128 \\\n","#         --print-freq=81920 \\\n","#         --print-time \\\n","#         --test-mini-batch-size=16384 \\\n","#         --test-num-workers=4 \\\n","#         --test-freq=655360\n","\n","#     python dlrm_s_pytorch.py \\\n","#         --arch-sparse-feature-size=16 \\\n","#         --arch-mlp-bot=\"13-512-256-64-16\" \\\n","#         --arch-mlp-top=\"512-256-1\" \\\n","#         --data-generation=dataset \\\n","#         --data-set=kaggle \\\n","#         --raw-data-file=./input/train_last_100k_as_example.txt \\\n","#         --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz \\\n","#         --load-model=./model/model_example.pt \\\n","#         --dump-json-file=./result/result_example.json \\\n","#         --test-data-split=test \\\n","#         --inference-only \\\n","#         --data-randomize=total \\\n","#         --lr-num-warmup-steps=0 \\\n","#         --lr-decay-start-step=0 \\\n","#         --loss-function=bce \\\n","#         --round-targets=True \\\n","#         --mlperf-logging \\\n","#         --nepochs=1 \\\n","#         --learning-rate=0.1 \\\n","#         --mini-batch-size=128 \\\n","#         --print-freq=81920 \\\n","#         --print-time \\\n","#         --test-mini-batch-size=16384 \\\n","#         --test-num-workers=4 \\\n","#         --test-freq=655360\n","\n","#     python dlrm_s_pytorch.py \\\n","#         --arch-sparse-feature-size=16 \\\n","#         --arch-mlp-bot=\"13-512-256-64-16\" \\\n","#         --arch-mlp-top=\"512-256-1\" \\\n","#         --data-generation=dataset \\\n","#         --data-set=kaggle \\\n","#         --raw-data-file=./input/train_last_100k_as_example.txt \\\n","#         --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz \\\n","#         --load-model=./model/model_example.pt \\\n","#         --dump-json-file=./result/result_example_val.json \\\n","#         --test-data-split=val \\\n","#         --inference-only \\\n","#         --data-randomize=total \\\n","#         --lr-num-warmup-steps=0 \\\n","#         --lr-decay-start-step=0 \\\n","#         --loss-function=bce \\\n","#         --round-targets=True \\\n","#         --mlperf-logging \\\n","#         --nepochs=1 \\\n","#         --learning-rate=0.1 \\\n","#         --mini-batch-size=128 \\\n","#         --print-freq=81920 \\\n","#         --print-time \\\n","#         --test-mini-batch-size=16384 \\\n","#         --test-num-workers=4 \\\n","#         --test-freq=655360\n","# done"],"metadata":{"id":"ih9ro_rngFB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 2: Apply VAD"],"metadata":{"id":"DWHEr63Eq2rq"}},{"cell_type":"code","source":["# %cd /content/gdrive/MyDrive/your_folder/VAD/"],"metadata":{"id":"Pha7Ltc_23DX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import codecs, json\n","import numpy as np\n","import tensorflow as tf\n","\n","def get_y_and_p(file_name):\n","    obj_text = codecs.open(file_name, 'r', encoding='utf-8').read()\n","    data = json.loads(obj_text)\n","    y = np.squeeze(np.array(data['targets']))\n","    p = np.squeeze(np.array(data['scores']))\n","    return y, p\n","\n","def assert_test_val_different(y_test, y_val):\n","    # make sure test data and val data are indeed different (i.e. no bug when do model inference)\n","    if len(y_test) == len(y_val):\n","        assert np.sum(y_test == y_val) < len(y_val)\n","\n","def read_data_from_json(\n","    json_name, \n","    json_name_val, \n","    num_group,\n","    alphas_zero,\n","):\n","\n","    def get_test_and_val(y_1, p_1, y_2, p_2):\n","        assert_test_val_different(y_1, y_2)\n","        y = np.hstack((y_1, y_2))\n","        p = np.hstack((p_1, p_2))\n","        num_examples = y.shape[0]\n","        num_examples_val = num_examples // 10\n","        num_examples -= num_examples_val\n","        return y[:num_examples], p[:num_examples], y[num_examples:], p[num_examples:]\n","\n","    y_1, p_1 = get_y_and_p(json_name.format(1))\n","    y_2, p_2 = get_y_and_p(json_name_val.format(1))\n","    y, p, y_val, p_val = get_test_and_val(y_1, p_1, y_2, p_2)\n","    num_examples = y.shape[0]\n","    num_examples_val = y_val.shape[0]\n","\n","    p_predicted_subgroup = np.zeros((num_examples, num_group))\n","    y_predicted_subgroup = np.zeros((num_examples, num_group))\n","    after_selection_calibration_val = {}\n","    for alpha_zero in alphas_zero:\n","        alpha_key = int(1000 * alpha_zero)\n","        after_selection_calibration_val[alpha_key] = np.zeros(num_group)\n","    \n","    for i in range(num_group):\n","        y_1, p_1 = get_y_and_p(json_name.format(i+1))\n","        y_2, p_2 = get_y_and_p(json_name_val.format(i+1))\n","        y, p, y_val, p_val = get_test_and_val(y_1, p_1, y_2, p_2)\n","        y_predicted_subgroup[:, i] = y\n","        p_predicted_subgroup[:, i] = p\n","        for alpha_zero in alphas_zero:\n","            ind_val = np.argpartition(p_val, -int(num_examples_val * alpha_zero))[-int(num_examples_val * alpha_zero):]\n","            after_selection_calibration_val[int(1000 * alpha_zero)][i] = np.sum(p_val[ind_val]) / np.sum(y_val[ind_val])\n","\n","    # make sure data is correct\n","    for i in range(num_group):\n","        assert np.sum(y_predicted_subgroup[:,0] == y_predicted_subgroup[:,i]) == num_examples\n","\n","    return p_predicted_subgroup, y_predicted_subgroup[:, 0], after_selection_calibration_val\n","\n","def get_ood_construct_model_predictions():\n","    json_name_nn_select = './dlrm/result/result_model_select.json'\n","    json_name_nn_select_val = './dlrm/result/result_model_select_val.json'\n","    y_1, p_1 = get_y_and_p(json_name_nn_select)\n","    y_2, p_2 = get_y_and_p(json_name_nn_select_val)\n","    assert_test_val_different(y_1, y_2)\n","    y = np.hstack((y_1, y_2))\n","    p = np.hstack((p_1, p_2))\n","    return y, p\n","\n","json_name = \"./dlrm/result/result_model_{}.json\"\n","json_name_val = \"./dlrm/result/result_model_{}_val.json\"\n","\n","# The paper replicates the experiment 40 times, so in order to reproduce the results,\n","# you should train 40 * 2 = 80 models and set num_group = 80\n","num_group = 4\n","alphas_zero = [0.02, 0.05, 0.1]\n","\n","bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n","ood_y, ood_p = get_ood_construct_model_predictions()\n","\n","p_predicted_subgroup_original, y_original, after_selection_calibration_val = read_data_from_json(\n","    json_name, \n","    json_name_val, \n","    num_group,\n","    alphas_zero,\n",")"],"metadata":{"id":"AD2z1ydMq-oL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_test_data(\n","    p_predicted_subgroup, \n","    y,\n","    do_p_selection,\n","    ood_p,\n","    ood_y,\n","):\n","    num_examples = p_predicted_subgroup.shape[0]\n","    assert np.sum(ood_y[:num_examples] == y) == len(y)\n","    selected_ind = list(range(num_examples))\n","    if do_p_selection:\n","        selected_ind = []\n","        for i in range(num_examples):\n","            select = np.random.binomial(1, 1 - ood_p[i])\n","            if select > 0.5:\n","                selected_ind.append(i)\n","    selected_ind = np.array(selected_ind)\n","    print(f\"Selected {len(selected_ind)} samples after test pre-process ({int(100 * len(selected_ind) // num_examples)}% data)\")\n","    print(\"After pre-process test positive ratio: \", np.sum(y[selected_ind]) / len(selected_ind))\n","    print()\n","    return p_predicted_subgroup[selected_ind], y[selected_ind]\n","\n","do_p_selection = False\n","num_group_bootstrap = 2\n","np.random.seed(1)\n","\n","result_keys = [\n","    'total_calibration',\n","    'positive_ratio_after_selection',\n","    'Vanilla',\n","    'Validation',\n","    'Validation_50',\n","    'Validation_100',\n","    'DBLL',\n","    'VAD',\n","    'VAD prob',\n","    'Ensemble',\n","    'Lambda logit',\n","    'Lambda prob',\n","    'Mu logit',\n","    'Mu prob',\n","    'Log Loss Logit Improvement',\n","    'Log Loss Prob Improvement',\n","]\n","\n","p_predicted_subgroup, y = preprocess_test_data(\n","    p_predicted_subgroup_original, \n","    y_original,\n","    do_p_selection,\n","    ood_p,\n","    ood_y,\n",")"],"metadata":{"id":"WrAYU62YxuTA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from VAD_util import calculate_lambda, prediction_transformation\n","\n","def generate_report(\n","    y, \n","    p_predicted_subgroup, \n","    after_selection_calibration_val, \n","    alpha, \n","    print_individual_result,\n","    num_group_bootstrap,\n","):\n","    def append_result(result, curr):\n","        for result_key in result_keys:\n","            result[result_key].append(curr[result_key])\n","        return result\n","\n","    num_examples, num_group = p_predicted_subgroup.shape\n","    num_examples_test_val = num_examples // 10\n","    num_examples -= num_examples_test_val\n","    p_predicted_subgroup_test_val = p_predicted_subgroup[:num_examples_test_val, :]\n","    p_predicted_subgroup = p_predicted_subgroup[num_examples_test_val:, :]\n","    y = y[num_examples_test_val:]\n","    assert num_examples == p_predicted_subgroup.shape[0]\n","\n","    result = {}\n","    for result_key in result_keys:\n","        result[result_key] = []\n","\n","    p_avg = np.mean(p_predicted_subgroup, axis=1)\n","\n","    for i in range(num_group):\n","        if i >= num_group // num_group_bootstrap:\n","            continue\n","        p = p_predicted_subgroup[:, i]\n","        p_next = p_predicted_subgroup[:, (i + num_group // num_group_bootstrap) % num_group] \n","        ind = np.argpartition(p, -int(num_examples * alpha))[-int(num_examples * alpha):]\n","        p_ground_truth = p_avg\n","\n","        pos_rate = np.sum(y) / num_examples\n","        NE_denominator = - (pos_rate * np.log(pos_rate) + (1 - pos_rate) * np.log(1 - pos_rate))\n","\n","        p_predicted_subgroup_test_val_choosen = np.zeros((num_examples_test_val, num_group_bootstrap))\n","        for j in range(num_group_bootstrap):\n","            p_predicted_subgroup_test_val_choosen[:, j] = p_predicted_subgroup_test_val[:, (i + j * num_group // num_group_bootstrap) % num_group]\n","        lambda_p_logit = calculate_lambda(p_predicted_subgroup_test_val_choosen, p_predicted_subgroup_test_val[:, i], 'logit')\n","        lambda_p_prob = calculate_lambda(p_predicted_subgroup_test_val_choosen, p_predicted_subgroup_test_val[:, i], 'probability')\n","        p_mean_test_val = np.mean(p_predicted_subgroup_test_val[:, i])\n","        p_mean_logit_test_val = np.mean(np.log(p_predicted_subgroup_test_val[:, i]/(1-p_predicted_subgroup_test_val[:, i])))\n","        refined_prediction_logit = prediction_transformation(p, ind, lambda_p_logit, 'logit', p_mean_test_val, p_mean_logit_test_val)\n","        refined_prediction_prob = prediction_transformation(p, ind, lambda_p_prob, 'probability', p_mean_test_val, p_mean_logit_test_val)\n","        log_loss_original = bce(y[ind], p[ind]).numpy()\n","        log_loss_logit = bce(y[ind], refined_prediction_logit).numpy()\n","        log_loss_prob = bce(y[ind], refined_prediction_prob).numpy()\n","        log_loss_logit_improve = (log_loss_logit - log_loss_original) / log_loss_original * 100\n","        log_loss_prob_improve = (log_loss_prob - log_loss_original) / log_loss_original * 100\n","\n","        curr = {\n","            'total_calibration': np.sum(p) / np.sum(y),\n","            'positive_ratio_after_selection': np.sum(y[ind]) / len(y[ind]),\n","            'Vanilla': np.sum(p[ind]) / np.sum(y[ind]),\n","            'Validation': np.sum(p[ind]) / np.sum(y[ind]) / after_selection_calibration_val[20][i],\n","            'Validation_50': np.sum(p[ind]) / np.sum(y[ind]) / after_selection_calibration_val[50][i],\n","            'Validation_100': np.sum(p[ind]) / np.sum(y[ind]) / after_selection_calibration_val[100][i],\n","            'DBLL': np.sum(p_next[ind]) / np.sum(y[ind]),\n","            'VAD': np.sum(refined_prediction_logit) / np.sum(y[ind]),\n","            'VAD prob': np.sum(refined_prediction_prob) / np.sum(y[ind]),\n","            'Ensemble': np.sum(p_ground_truth[ind]) / np.sum(y[ind]),\n","            'Lambda logit': lambda_p_logit,\n","            'Lambda prob': lambda_p_prob,\n","            'Mu logit': p_mean_logit_test_val,\n","            'Mu prob': p_mean_test_val,\n","            'Log Loss Logit Improvement': log_loss_logit_improve,\n","            'Log Loss Prob Improvement': log_loss_prob_improve,\n","        }\n","        result = append_result(result, curr)\n","        \n","        log_loss = bce(y, p).numpy()\n","\n","        if print_individual_result:\n","            print(\"LogLoss: \", log_loss)\n","            print(\"NE: \", log_loss / NE_denominator)\n","            for result_key in result_keys:\n","                print(result_key, \": \", curr[result_key])\n","            print()\n","\n","    print(\"Num Group: \", num_group)\n","    print(\"Alpha: \", alpha)\n","    for result_key in result_keys:\n","        result[result_key] = np.array(result[result_key])\n","        print(\"mean of \", result_key, \": \", np.mean(result[result_key]))\n","    print()\n","\n","    return result\n","\n","def generate_multiple_report(p_predicted_subgroup, y, after_selection_calibration_val):\n","    num_group = p_predicted_subgroup.shape[1]\n","    report = {}\n","    alphas = [0.02, 0.05, 0.1]\n","    for alpha in alphas:\n","        result = generate_report(\n","            y, \n","            p_predicted_subgroup, \n","            after_selection_calibration_val, \n","            alpha, \n","            False,\n","            num_group_bootstrap,\n","        )\n","        alpha_key = int(1000 * alpha)\n","        report[alpha_key] = {}\n","        for result_key in reported_result_keys:\n","            curr_result = result[result_key]\n","            total_num_result = curr_result.shape[0]\n","            report[alpha_key][result_key] = (np.mean(curr_result), np.std(curr_result) / np.sqrt(total_num_result))\n","    return report\n","\n","\n","reported_result_keys = [\n","    'Vanilla',\n","    'Validation',\n","    'DBLL',\n","    'VAD',\n","    'Ensemble',\n","]\n","\n","final_report = generate_multiple_report(p_predicted_subgroup, y, after_selection_calibration_val)\n"],"metadata":{"id":"RaSRfHBqy9Jc"},"execution_count":null,"outputs":[]}]}