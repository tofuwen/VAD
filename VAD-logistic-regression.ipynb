{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VAD-logistic-regression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOS4OdonEOowJsVAyemFqf0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"XLCwlGegVT-Z","executionInfo":{"status":"ok","timestamp":1643314168720,"user_tz":480,"elapsed":962597,"user":{"displayName":"Yewen Fan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08524649782816922526"}},"outputId":"c457546a-3a0d-4a9e-cb95-e7b746ee9e66","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["alpha:  0.02\n","mean of total_calibration_p :  0.9983254908541483\n","mean of total_calibration_y :  0.9986705139579292\n","mean of positive_ratio_after_selection :  0.4988100000000001\n","mean of positive_ratio_train :  0.27695766666666666\n","mean of positive_ratio_test :  0.2774246\n","mean of Ground truth :  1.0080126141538204\n","mean of calibration_y_true_p :  1.0037009963251953\n","mean of calibration_p_true_p :  0.997052125766921\n","mean of calibration_p_y_true_p :  0.9981110446276411\n","mean of Vanilla :  1.0891093958203024\n","mean of Vanilla_p :  1.080432238506638\n","mean of Validation :  1.0193602336925756\n","mean of Validation 0.05 :  1.0310746711722194\n","mean of Validation 0.1 :  1.0463811687616262\n","mean of VAD :  0.9980938623089092\n","mean of VAD_p :  0.9900952196009641\n","mean of VAD prob :  1.0062753353042684\n","mean of VAD prob_p :  0.9982116008150201\n","mean of DBLL :  1.0881368478543274\n","mean of Ensemble :  1.0888900880436532\n","mean of Lambda logit :  0.8420621728904076\n","mean of Lambda prob :  0.8416712005978488\n","mean of Mu logit :  -1.0106507392068786\n","mean of Mu prob :  0.2769414517450708\n","mean of Log Loss Logit Improvement :  -0.3807099611508412\n","mean of Log Loss Prob Improvement :  -0.4091452339870069\n","\n","alpha:  0.05\n","mean of total_calibration_p :  0.9983254908541483\n","mean of total_calibration_y :  0.9986705139579292\n","mean of positive_ratio_after_selection :  0.4631660000000001\n","mean of positive_ratio_train :  0.27695766666666666\n","mean of positive_ratio_test :  0.2774246\n","mean of Ground truth :  1.0035841610167902\n","mean of calibration_y_true_p :  1.0018321273221047\n","mean of calibration_p_true_p :  0.9970873428933358\n","mean of calibration_p_y_true_p :  0.9974456598073956\n","mean of Vanilla :  1.0767258699834323\n","mean of Vanilla_p :  1.0728893905978585\n","mean of Validation :  1.0078687106390989\n","mean of Validation 0.05 :  1.0193479812984443\n","mean of Validation 0.1 :  1.0345655512610552\n","mean of VAD :  0.9940703560398608\n","mean of VAD_p :  0.9905729910557952\n","mean of VAD prob :  1.0029469880723565\n","mean of VAD prob_p :  0.9994084297330151\n","mean of DBLL :  1.0773960291519424\n","mean of Ensemble :  1.076867839021218\n","mean of Lambda logit :  0.8433640576172728\n","mean of Lambda prob :  0.8433000090725034\n","mean of Mu logit :  -1.0106507392068786\n","mean of Mu prob :  0.2769414517450708\n","mean of Log Loss Logit Improvement :  -0.2811803268381134\n","mean of Log Loss Prob Improvement :  -0.3020459730633772\n","\n","alpha:  0.1\n","mean of total_calibration_p :  0.9983254908541483\n","mean of total_calibration_y :  0.9986705139579292\n","mean of positive_ratio_after_selection :  0.432392\n","mean of positive_ratio_train :  0.27695766666666666\n","mean of positive_ratio_test :  0.2774246\n","mean of Ground truth :  1.00203373805045\n","mean of calibration_y_true_p :  0.9992448358960807\n","mean of calibration_p_true_p :  0.9973855677945551\n","mean of calibration_p_y_true_p :  0.999418064652512\n","mean of Vanilla :  1.066764870245241\n","mean of Vanilla_p :  1.0646224502612145\n","mean of Validation :  0.998810224575592\n","mean of Validation 0.05 :  1.0101169190687476\n","mean of Validation 0.1 :  1.0251631394809082\n","mean of VAD :  0.9929252607222718\n","mean of VAD_p :  0.9909535663533124\n","mean of VAD prob :  1.0015548518940935\n","mean of VAD prob_p :  0.999560394623658\n","mean of DBLL :  1.0660231121893957\n","mean of Ensemble :  1.066680788449372\n","mean of Lambda logit :  0.844968024423794\n","mean of Lambda prob :  0.8446348150947363\n","mean of Mu logit :  -1.0106507392068786\n","mean of Mu prob :  0.2769414517450708\n","mean of Log Loss Logit Improvement :  -0.19458807829912975\n","mean of Log Loss Prob Improvement :  -0.2094406201745706\n","\n"]}],"source":["import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import log_loss\n","from VAD_util import calculate_lambda, prediction_transformation\n","\n","# fix random seed so it's easier to reproduce the result\n","np.random.seed(1234)\n","\n","num_features = 20\n","sigma = 0.1\n","num_examples = 10000\n","num_examples_train = 3000\n","alphas_val = [0.02, 0.05, 0.1]\n","num_simulation = 1000\n","S_GROUP = 2\n","\n","result_keys = [\n","    'total_calibration_p',\n","    'total_calibration_y',\n","    'positive_ratio_after_selection',\n","    'positive_ratio_train',\n","    'positive_ratio_test',\n","    'Ground truth',\n","    'calibration_y_true_p',\n","    'calibration_p_true_p',\n","    'calibration_p_y_true_p',\n","    'Vanilla',\n","    'Vanilla_p',\n","    'Validation',\n","    'Validation 0.05',\n","    'Validation 0.1',\n","    'VAD',\n","    'VAD_p',\n","    'VAD prob',\n","    'VAD prob_p',\n","    'DBLL',\n","    'Ensemble',\n","    'Lambda logit',\n","    'Lambda prob',\n","    'Mu logit',\n","    'Mu prob',\n","    'Log Loss Logit Improvement',\n","    'Log Loss Prob Improvement',\n","]\n","\n","def generate_data(num_examples, mu):\n","    x = np.random.normal(mu, sigma, (num_examples, num_features))\n","    p = 1 / (1 + np.exp(- np.sum(x, axis=1)))\n","    y = np.random.binomial(1, p)\n","    return x, y, p\n","\n","def bootstrap_data(x, y):\n","    num_examples = x.shape[0]\n","    ind = np.random.choice(np.arange(num_examples), num_examples, replace=True)\n","    return x[ind], y[ind]\n","\n","def VAD_method(num_simulation, num_group, method, alpha, train_data, val_data, test_data, test_val_data):\n","\n","    def calculate_logit(X, clf):\n","        return np.squeeze(X @ clf.coef_.T + clf.intercept_)\n","\n","    def append_result(result, curr):\n","        for result_key in result_keys:\n","            result[result_key].append(curr[result_key])\n","        return result\n","\n","    assert method in ['bootstrap', 'seperate_data']\n","    x_train_array, y_train_array, p_train_array = train_data\n","    x_val_array, y_val_array, p_val_array = val_data\n","    x_test_val_array, _, _ = test_val_data \n","    x_test_array, y_test_array, p_test_array = test_data \n","\n","    result = {}\n","    for result_key in result_keys:\n","        result[result_key] = []\n","\n","    for k in range(num_simulation):\n","        x_train, y_train, p_train = x_train_array[k], y_train_array[k], p_train_array[k]\n","        x_val, y_val, p_val = x_val_array[k], y_val_array[k], p_val_array[k]\n","        x_test, y_test, p_test = x_test_array[k], y_test_array[k], p_test_array[k]\n","        x_test_val = x_test_val_array[k]\n","        clf = LogisticRegression(penalty='none', random_state=0).fit(x_train, y_train)\n","        p_predicted = clf.predict_proba(x_test)[:, 1]\n","        p_predicted_test_val = clf.predict_proba(x_test_val)[:, 1]\n","        p_predicted_val = clf.predict_proba(x_val)[:, 1]\n","        logit_predicted = calculate_logit(x_test, clf)\n","        logit_predicted_test_val = calculate_logit(x_test_val, clf)\n","        num_examples_test_val = x_test_val.shape[0]\n","        num_examples_test = x_test.shape[0]\n","        p_predicted_subgroup = np.zeros((num_examples_test_val, num_group))\n","        logit_predicted_subgroup = np.zeros((num_examples_test_val, num_group))\n","        for i in range(num_group):\n","            x_train_bootstrap, y_train_bootstrap = bootstrap_data(x_train, y_train)\n","            clf_i = LogisticRegression(penalty='none', random_state=0).fit(x_train_bootstrap, y_train_bootstrap)\n","            p_predicted_subgroup[:, i] = clf_i.predict_proba(x_test_val)[:, 1]\n","            logit_predicted_subgroup[:, i] = calculate_logit(x_test_val, clf_i)\n","\n","        x_train_bootstrap, y_train_bootstrap = bootstrap_data(x_train, y_train)\n","        clf_next = LogisticRegression(penalty='none', random_state=0).fit(x_train_bootstrap, y_train_bootstrap)\n","        p_predicted_next = clf_next.predict_proba(x_test)[:, 1]\n","        p_predicted_ensemble = np.zeros(num_examples_test)\n","        for i in range(num_ensemble):\n","            x_train_bootstrap, y_train_bootstrap = bootstrap_data(x_train, y_train)\n","            clf_i = LogisticRegression(penalty='none', random_state=0).fit(x_train_bootstrap, y_train_bootstrap)\n","            p_predicted_i = clf_i.predict_proba(x_test)[:, 1]\n","            p_predicted_ensemble += p_predicted_i / num_ensemble\n","\n","        lambda_p_logit = calculate_lambda(p_predicted_subgroup, p_predicted_test_val, 'logit', logit_predicted_subgroup, logit_predicted_test_val)\n","        lambda_p_prob = calculate_lambda(p_predicted_subgroup, p_predicted_test_val, 'probability', logit_predicted_subgroup, logit_predicted_test_val)\n","\n","        choose_num_examples = int(num_examples_test * alpha)\n","        ind = np.argpartition(p_predicted, -choose_num_examples)[-choose_num_examples:]\n","        val_calibration_report = {}\n","        for alpha_val in alphas_val:\n","            choose_num_examples_val = int(num_examples_test_val * alpha_val)\n","            ind_val = np.argpartition(p_predicted_val, -choose_num_examples_val)[-choose_num_examples_val:]\n","            val_calibration_report[int(alpha_val * 1000)] = np.sum(p_predicted_val[ind_val]) / np.sum(y_val[ind_val])\n","        ind_true_p =  np.argpartition(p_test, -choose_num_examples)[-choose_num_examples:]\n","\n","        log_loss = np.sum(-(y_test[ind] * np.log(p_predicted[ind]) + (1 - y_test[ind]) * np.log(1 - p_predicted[ind])))\n","        refined_prediction_logit = prediction_transformation(p_predicted, ind, lambda_p_logit, 'logit', np.mean(p_predicted_test_val), np.mean(logit_predicted_test_val), logit_predicted)\n","        refined_prediction_prob = prediction_transformation(p_predicted, ind, lambda_p_prob, 'probability', np.mean(p_predicted_test_val), np.mean(logit_predicted_test_val), logit_predicted)\n","        refined_log_loss_logit = np.sum(-(y_test[ind] * np.log(refined_prediction_logit) + (1 - y_test[ind]) * np.log(1 - refined_prediction_logit)))\n","        refined_log_loss_prob = np.sum(-(y_test[ind] * np.log(refined_prediction_prob) + (1 - y_test[ind]) * np.log(1 - refined_prediction_prob)))\n","\n","        curr = {\n","            'total_calibration_p': np.sum(p_predicted) / np.sum(p_test),\n","            'total_calibration_y': np.sum(p_predicted) / np.sum(y_test),\n","            'positive_ratio_after_selection': np.sum(y_test[ind]) / len(y_test[ind]),\n","            'positive_ratio_train': np.sum(y_train) / len(y_train),\n","            'positive_ratio_test': np.sum(y_test) / len(y_test),\n","            'Ground truth': np.sum(p_test[ind]) / np.sum(y_test[ind]),\n","            'calibration_y_true_p': np.sum(p_predicted[ind_true_p]) / np.sum(y_test[ind_true_p]),\n","            'calibration_p_true_p': np.sum(p_predicted[ind_true_p]) / np.sum(p_test[ind_true_p]),\n","            'calibration_p_y_true_p': np.sum(y_test[ind_true_p]) / np.sum(p_test[ind_true_p]),\n","            'Vanilla': np.sum(p_predicted[ind]) / np.sum(y_test[ind]),\n","            'Vanilla_p': np.sum(p_predicted[ind]) / np.sum(p_test[ind]),\n","            'Validation': np.sum(p_predicted[ind]) / np.sum(y_test[ind]) / val_calibration_report[20],\n","            'Validation 0.05': np.sum(p_predicted[ind]) / np.sum(y_test[ind]) / val_calibration_report[50],\n","            'Validation 0.1': np.sum(p_predicted[ind]) / np.sum(y_test[ind]) / val_calibration_report[100],\n","            'VAD': np.sum(refined_prediction_logit) / np.sum(y_test[ind]),\n","            'VAD_p': np.sum(refined_prediction_logit) / np.sum(p_test[ind]),\n","            'VAD prob': np.sum(refined_prediction_prob) / np.sum(y_test[ind]),\n","            'VAD prob_p': np.sum(refined_prediction_prob) / np.sum(p_test[ind]),\n","            'DBLL': np.sum(p_predicted_next[ind]) / np.sum(y_test[ind]),\n","            'Ensemble': np.sum(p_predicted_ensemble[ind]) / np.sum(y_test[ind]),\n","            'Lambda logit': lambda_p_logit,\n","            'Lambda prob': lambda_p_prob,\n","            'Mu logit': np.mean(logit_predicted_test_val),\n","            'Mu prob': np.mean(p_predicted_test_val),\n","            'Log Loss Logit Improvement': (refined_log_loss_logit - log_loss) / log_loss * 100,\n","            'Log Loss Prob Improvement': (refined_log_loss_prob - log_loss) / log_loss * 100,\n","        }\n","        result = append_result(result, curr)\n","\n","    print(\"alpha: \", alpha)\n","    for result_key in result_keys:\n","        result[result_key] = np.array(result[result_key])\n","        print(\"mean of\", result_key, \": \", np.mean(result[result_key]))\n","    print()\n","\n","    return result\n","\n","def generate_data_array(num_simulation, num_examples, mu):\n","    x_array = []\n","    y_array = []\n","    p_array = []\n","    for k in range(num_simulation):\n","        x, y, p = generate_data(num_examples, mu)\n","        x_array.append(x)\n","        y_array.append(y)\n","        p_array.append(p)\n","    return (x_array, y_array, p_array)\n","\n","\n","mu = -0.05\n","mu_train = -0.05\n","num_ensemble = 10\n","train_data = generate_data_array(num_simulation, num_examples_train, mu_train)\n","val_data = generate_data_array(num_simulation, num_examples_train, mu_train)\n","test_data = generate_data_array(num_simulation, num_examples, mu)\n","test_val_data = generate_data_array(num_simulation, num_examples, mu)\n","\n","report = {}\n","reported_result_keys = [\n","    'Vanilla',\n","    'Validation',\n","    'Validation 0.05',\n","    'Validation 0.1',\n","    'VAD',\n","    'VAD prob',\n","    'DBLL',\n","    'Ensemble',\n","]\n","alphas = [0.02, 0.05, 0.1]\n","for alpha in alphas:\n","    result = VAD_method(num_simulation, S_GROUP, 'bootstrap', alpha, train_data, val_data, test_data, test_val_data)\n","    alpha_key = int(alpha * 1000)\n","    report[alpha_key] = {}\n","    for result_key in reported_result_keys:\n","        curr_result = result[result_key]\n","        total_num_result = curr_result.shape[0]\n","        report[alpha_key][result_key] = (np.mean(curr_result), np.std(curr_result) / np.sqrt(total_num_result))"]},{"cell_type":"code","source":[""],"metadata":{"id":"0tgJFis4GL_d"},"execution_count":null,"outputs":[]}]}